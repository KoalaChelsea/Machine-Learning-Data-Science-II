{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification - MNIST Dataset\n",
    "Solve the same problem as MLP\\3_ImageClassification\\example_MNIST.py but using a CNN.\n",
    "\n",
    "New Code\n",
    "- Reshaping to input into a Conv Layer.\n",
    "- Convolutional and Pooling Layers. Flatten operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 0.2116 - accuracy: 0.9337 - val_loss: 3.1678 - val_accuracy: 0.1137\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 18s 308us/step - loss: 0.0635 - accuracy: 0.9802 - val_loss: 2.5937 - val_accuracy: 0.2306\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.0459 - accuracy: 0.9860 - val_loss: 0.7573 - val_accuracy: 0.7160\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 0.0365 - accuracy: 0.9888 - val_loss: 0.1755 - val_accuracy: 0.9473\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.0286 - accuracy: 0.9914 - val_loss: 0.0481 - val_accuracy: 0.9840\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 0.0258 - accuracy: 0.9919 - val_loss: 0.0316 - val_accuracy: 0.9893\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0311 - val_accuracy: 0.9904\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 19s 317us/step - loss: 0.0179 - accuracy: 0.9945 - val_loss: 0.0342 - val_accuracy: 0.9884\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.0163 - accuracy: 0.9952 - val_loss: 0.0344 - val_accuracy: 0.9901\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0312 - val_accuracy: 0.9909\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.0295 - val_accuracy: 0.9911\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0289 - val_accuracy: 0.9912\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 18s 305us/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0347 - val_accuracy: 0.9898\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0288 - val_accuracy: 0.9912\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0351 - val_accuracy: 0.9890\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0351 - val_accuracy: 0.9902\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0327 - val_accuracy: 0.9900\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 19s 315us/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0340 - val_accuracy: 0.9903\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0404 - val_accuracy: 0.9890\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0421 - val_accuracy: 0.9883\n",
      "10000/10000 [==============================] - 1s 122us/step\n",
      "Final accuracy on validations set: 98.83000254631042 %\n"
     ]
    }
   ],
   "source": [
    "# %% --------------------------------------- Imports -------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Conv2D, Flatten, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import keras\n",
    "# %% --------------------------------------- Set-Up --------------------------------------------------------------------\n",
    "# SEED = 42\n",
    "# os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)\n",
    "\n",
    "# %% ----------------------------------- Hyper Parameters --------------------------------------------------------------\n",
    "LR = 1e-3\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "DROPOUT = 0.5\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "# %% -------------------------------------- Data Prep ------------------------------------------------------------------\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Reshapes to (n_examples, n_channels, height_pixels, width_pixels)\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "# x_train, x_test = x_train.reshape(len(x_train), 1, 28, 28), x_test.reshape(len(x_test), 1, 28, 28)\n",
    "# y_train, y_test = to_categorical(y_train, num_classes=10), to_categorical(y_test, num_classes=10)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "# %% -------------------------------------- Training Prep ----------------------------------------------------------\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3,3),  activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(32, (3,3), activation=\"relu\"),\n",
    "    #The Batch Normalization normalizes the outputs from the hidden activation functions. \n",
    "    # This helps with neuron imbalance and can speed training significantly.\n",
    "    BatchNormalization(),\n",
    "    AveragePooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(400, activation=\"tanh\"),\n",
    "    Dropout(DROPOUT),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=Adam(lr=LR), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# %% -------------------------------------- Training Loop ----------------------------------------------------------\n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(x_test, y_test))\n",
    "\n",
    "# %% ------------------------------------------ Final test -------------------------------------------------------------\n",
    "print(\"Final accuracy on validations set:\", 100*model.evaluate(x_test, y_test)[1], \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n",
      "Epoch 0 | Train Loss 0.31078, Train Acc 92.58 - Test Loss 0.64011, Test Acc 89.90\n",
      "Epoch 1 | Train Loss 0.08680, Train Acc 97.53 - Test Loss 0.10478, Test Acc 96.94\n",
      "Epoch 2 | Train Loss 0.06868, Train Acc 98.03 - Test Loss 0.04896, Test Acc 98.59\n",
      "Epoch 3 | Train Loss 0.06568, Train Acc 98.16 - Test Loss 0.10479, Test Acc 97.21\n",
      "Epoch 4 | Train Loss 0.06351, Train Acc 98.22 - Test Loss 1.27813, Test Acc 78.31\n",
      "Epoch 5 | Train Loss 0.07397, Train Acc 97.97 - Test Loss 0.65000, Test Acc 89.57\n",
      "Epoch 6 | Train Loss 0.07231, Train Acc 98.10 - Test Loss 0.09979, Test Acc 98.23\n",
      "Epoch 7 | Train Loss 0.06954, Train Acc 98.31 - Test Loss 0.07762, Test Acc 98.51\n",
      "Epoch 8 | Train Loss 0.10667, Train Acc 97.53 - Test Loss 0.56933, Test Acc 91.55\n",
      "Epoch 9 | Train Loss 0.08778, Train Acc 97.96 - Test Loss 0.06872, Test Acc 98.44\n",
      "Epoch 10 | Train Loss 0.06481, Train Acc 98.42 - Test Loss 0.11595, Test Acc 97.56\n",
      "Epoch 11 | Train Loss 0.07399, Train Acc 98.26 - Test Loss 0.08427, Test Acc 98.13\n",
      "Epoch 12 | Train Loss 0.06364, Train Acc 98.44 - Test Loss 0.08592, Test Acc 98.51\n",
      "Epoch 13 | Train Loss 0.06083, Train Acc 98.50 - Test Loss 0.07253, Test Acc 98.67\n",
      "Epoch 14 | Train Loss 0.06330, Train Acc 98.57 - Test Loss 0.10119, Test Acc 97.73\n",
      "Epoch 15 | Train Loss 0.05881, Train Acc 98.62 - Test Loss 0.13136, Test Acc 97.86\n",
      "Epoch 16 | Train Loss 0.06124, Train Acc 98.67 - Test Loss 0.17101, Test Acc 98.28\n",
      "Epoch 17 | Train Loss 0.07221, Train Acc 98.62 - Test Loss 0.27544, Test Acc 97.66\n",
      "Epoch 18 | Train Loss 0.08780, Train Acc 98.51 - Test Loss 0.56496, Test Acc 96.88\n",
      "Epoch 19 | Train Loss 0.09228, Train Acc 98.60 - Test Loss 0.09898, Test Acc 98.80\n"
     ]
    }
   ],
   "source": [
    "# %% --------------------------------------- Imports -------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# %% --------------------------------------- Set-Up --------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# %% ----------------------------------- Hyper Parameters --------------------------------------------------------------\n",
    "LR = 5e-2\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "\n",
    "# %% -------------------------------------- CNN Class ------------------------------------------------------------------\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, 3)  # output (n_examples, 26, 26, 16)\n",
    "        self.convnorm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(2)  # output (n_examples, 13, 13, 16)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 3)  # output (n_examples, 11, 11, 32)\n",
    "        self.convnorm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.AveragePooling2D(2)  # output (n_examples, 5, 5, 32)\n",
    "        self.flatten = tf.keras.layers.Flatten()  # input will be flattened to (n_examples, 32 * 5 * 5)\n",
    "        self.linear1 = tf.keras.layers.Dense(400)\n",
    "        self.linear1_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.linear2 = tf.keras.layers.Dense(10)\n",
    "        self.act = tf.nn.relu\n",
    "        self.drop = DROPOUT\n",
    "        self.training = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.pool1(self.convnorm1(self.act(self.conv1(x)), training=self.training))\n",
    "        x = self.flatten(self.pool2(self.convnorm2(self.act(self.conv2(x)), training=self.training)))\n",
    "        x = tf.nn.dropout(self.linear1_bn(self.act(self.linear1(x)), training=self.training), self.drop)\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "# %% -------------------------------------- Data Prep ------------------------------------------------------------------\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Reshapes to (n_examples, height, width, n_channels)\n",
    "x_train, x_test = tf.reshape(x_train, (len(x_train), 28, 28, 1), ), tf.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "x_train, x_test = tf.dtypes.cast(x_train, tf.float32), tf.dtypes.cast(x_test, tf.float32)\n",
    "y_train, y_test = tf.convert_to_tensor(y_train), tf.convert_to_tensor(y_test)\n",
    "\n",
    "# %% -------------------------------------- Training Prep ----------------------------------------------------------\n",
    "model = CNN()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "@tf.function\n",
    "def train(x, y):\n",
    "    model.training = True\n",
    "    model.drop = DROPOUT\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = criterion(y, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, logits)\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "@tf.function\n",
    "def eval(x, y):\n",
    "    model.training = False\n",
    "    model.drop = 0\n",
    "    logits = model(x)\n",
    "    loss = criterion(y, logits)\n",
    "    test_loss(loss)\n",
    "    test_accuracy(y, logits)\n",
    "\n",
    "\n",
    "# %% -------------------------------------- Training Loop ----------------------------------------------------------\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    for batch in range(len(x_train)//BATCH_SIZE + 1):\n",
    "        inds = slice(batch*BATCH_SIZE, (batch+1)*BATCH_SIZE)\n",
    "        train(x_train[inds], y_train[inds])\n",
    "\n",
    "    eval(x_test, y_test)\n",
    "\n",
    "    print(\"Epoch {} | Train Loss {:.5f}, Train Acc {:.2f} - Test Loss {:.5f}, Test Acc {:.2f}\".format(\n",
    "        epoch, train_loss.result(), train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n",
    "    train_loss.reset_states(); train_accuracy.reset_states(); test_loss.reset_states(); test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n",
      "Epoch 0 | Train Loss 0.06107, Train Acc 97.23 - Test Loss 0.08982, Test Acc 97.41\n",
      "Epoch 1 | Train Loss 0.02204, Train Acc 98.22 - Test Loss 0.06169, Test Acc 98.16\n",
      "Epoch 2 | Train Loss 0.01656, Train Acc 98.68 - Test Loss 0.05040, Test Acc 98.45\n",
      "Epoch 3 | Train Loss 0.01354, Train Acc 98.89 - Test Loss 0.04367, Test Acc 98.62\n",
      "Epoch 4 | Train Loss 0.01199, Train Acc 99.05 - Test Loss 0.03996, Test Acc 98.68\n",
      "Epoch 5 | Train Loss 0.01040, Train Acc 99.15 - Test Loss 0.03718, Test Acc 98.79\n",
      "Epoch 6 | Train Loss 0.00926, Train Acc 99.30 - Test Loss 0.03570, Test Acc 98.86\n",
      "Epoch 7 | Train Loss 0.00830, Train Acc 99.40 - Test Loss 0.03363, Test Acc 99.03\n",
      "Epoch 8 | Train Loss 0.00764, Train Acc 99.48 - Test Loss 0.03258, Test Acc 99.04\n",
      "Epoch 9 | Train Loss 0.00698, Train Acc 99.49 - Test Loss 0.03209, Test Acc 99.08\n",
      "Epoch 10 | Train Loss 0.00665, Train Acc 99.56 - Test Loss 0.03093, Test Acc 99.01\n",
      "Epoch 11 | Train Loss 0.00594, Train Acc 99.60 - Test Loss 0.03072, Test Acc 98.99\n",
      "Epoch 12 | Train Loss 0.00563, Train Acc 99.66 - Test Loss 0.02955, Test Acc 99.10\n",
      "Epoch 13 | Train Loss 0.00531, Train Acc 99.69 - Test Loss 0.02903, Test Acc 99.14\n",
      "Epoch 14 | Train Loss 0.00494, Train Acc 99.69 - Test Loss 0.02914, Test Acc 99.07\n",
      "Epoch 15 | Train Loss 0.00474, Train Acc 99.71 - Test Loss 0.02877, Test Acc 99.12\n",
      "Epoch 16 | Train Loss 0.00446, Train Acc 99.75 - Test Loss 0.02793, Test Acc 99.09\n",
      "Epoch 17 | Train Loss 0.00414, Train Acc 99.79 - Test Loss 0.02747, Test Acc 99.09\n",
      "Epoch 18 | Train Loss 0.00386, Train Acc 99.80 - Test Loss 0.02783, Test Acc 99.12\n",
      "Epoch 19 | Train Loss 0.00370, Train Acc 99.82 - Test Loss 0.02822, Test Acc 99.09\n"
     ]
    }
   ],
   "source": [
    "# %% --------------------------------------- Imports -------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# %% --------------------------------------- Set-Up --------------------------------------------------------------------\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# %% ----------------------------------- Hyper Parameters --------------------------------------------------------------\n",
    "LR = 5e-2\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "\n",
    "# %% ----------------------------------- Helper Functions --------------------------------------------------------------\n",
    "def acc(x, y, return_labels=False):\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        pred_labels = np.argmax(logits.cpu().numpy(), axis=1)\n",
    "    if return_labels:\n",
    "        return pred_labels\n",
    "    else:\n",
    "        return 100*accuracy_score(y.cpu().numpy(), pred_labels)\n",
    "\n",
    "\n",
    "# %% -------------------------------------- CNN Class ------------------------------------------------------------------\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (3, 3))  # output (n_examples, 16, 26, 26)\n",
    "        self.convnorm1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))  # output (n_examples, 16, 13, 13)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3))  # output (n_examples, 32, 11, 11)\n",
    "        self.convnorm2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d((2, 2))  # output (n_examples, 32, 5, 5)\n",
    "        self.linear1 = nn.Linear(32*5*5, 400)  # input will be flattened to (n_examples, 32 * 5 * 5)\n",
    "        self.linear1_bn = nn.BatchNorm1d(400)\n",
    "        self.drop = nn.Dropout(DROPOUT)\n",
    "        self.linear2 = nn.Linear(400, 10)\n",
    "        self.act = torch.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.convnorm1(self.act(self.conv1(x))))\n",
    "        x = self.pool2(self.convnorm2(self.act(self.conv2(x))))\n",
    "        x = self.drop(self.linear1_bn(self.act(self.linear1(x.view(len(x), -1)))))\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "# %% -------------------------------------- Data Prep ------------------------------------------------------------------\n",
    "data_train = datasets.MNIST(root='.', train=True, download=True)\n",
    "# Reshapes to (n_examples, n_channels, height_pixels, width_pixels)\n",
    "x_train, y_train = data_train.data.view(len(data_train), 1, 28, 28).float().to(device), data_train.targets.to(device)\n",
    "x_train.requires_grad = True\n",
    "data_test = datasets.MNIST(root='.', train=False, download=True)\n",
    "x_test, y_test = data_test.data.view(len(data_test), 1, 28, 28).float().to(device), data_test.targets.to(device)\n",
    "\n",
    "# %% -------------------------------------- Training Prep ----------------------------------------------------------\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# %% -------------------------------------- Training Loop ----------------------------------------------------------\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    loss_train = 0\n",
    "    model.train()\n",
    "    for batch in range(len(x_train)//BATCH_SIZE + 1):\n",
    "        inds = slice(batch*BATCH_SIZE, (batch+1)*BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_train[inds])\n",
    "        loss = criterion(logits, y_train[inds])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(x_test)\n",
    "        loss = criterion(y_test_pred, y_test)\n",
    "        loss_test = loss.item()\n",
    "\n",
    "    print(\"Epoch {} | Train Loss {:.5f}, Train Acc {:.2f} - Test Loss {:.5f}, Test Acc {:.2f}\".format(\n",
    "        epoch, loss_train/BATCH_SIZE, acc(x_train, y_train), loss_test, acc(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
